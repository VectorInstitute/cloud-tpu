{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6376177f-d1e4-47b3-b59f-db6498f2a182",
   "metadata": {},
   "source": [
    "# Simple Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5349ea11-28d2-4607-b936-34d109ef22c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch-lightning.readthedocs.io/en/stable/model/train_model_basic.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f56eb8-62e7-45db-9611-5e36e735b11d",
   "metadata": {},
   "source": [
    "# XLA Configuration\n",
    "\n",
    "Before starting, instruct your python kernel to use the TPU accelerator by setting the `XRT_TPU_CONFIG` env var. We set TPU config using the address to host process for TPU cores on your VM.<br> `XLA_USE_BF16=1` instructs PyTorch to use the bFloat16 format, rather than Float32. This will maximize TPU performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79a46c33-ab36-4add-85d1-46751783908a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XRT_TPU_CONFIG=localservice;0;localhost:51011\n",
      "env: XLA_USE_BF16=1\n"
     ]
    }
   ],
   "source": [
    "%env XRT_TPU_CONFIG=localservice;0;localhost:51011\n",
    "%env XLA_USE_BF16=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c57ac6ef-f805-4617-ae47-f3a2baa37b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb4d043-eff1-448f-8253-a56bcae43c6f",
   "metadata": {},
   "source": [
    "# XLA Libraries\n",
    "Here, we import all of the required torch XLA libraries in order to use the TPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4daed8aa-64b4-47d5-805e-a5150f4947cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.debug.metrics as met\n",
    "import torch_xla.distributed.parallel_loader as ploader\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch_xla.utils.utils as xu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dc8ce7-0a86-4f66-b141-139f7d2033b6",
   "metadata": {},
   "source": [
    "# Create a Pytorch Lightning Module\n",
    "Below is a simple Autoencoder model configured from the LightningModule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "872c7016-d8fd-41ee-9d81-4c0eb9ee93c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.l1(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.l1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1368347-14bf-4d7d-be3c-3d7284a035e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, batch, batch_idx, device):\n",
    "        # training_step defines the train loop.\n",
    "        x, y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b8c7ac-e3cc-4ad7-8fe6-e7ce4f9f8673",
   "metadata": {},
   "source": [
    "# Distributed Sampling\n",
    "In order to use parallelization, we must set up a `DistributedSampler` from torch utils. This sampler is passed into our dataloader and allows us to sample subsets of the data for multiprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "068e17c1-f969-4de5-b393-043a4b3644ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.distributed import DistributedSampler\n",
    "train_dataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_sampler = DistributedSampler(\n",
    "        train_dataset,\n",
    "        num_replicas=xm.xrt_world_size(),\n",
    "        rank=xm.get_ordinal(),\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, sampler=train_sampler, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b297d86b-9b4d-4f3a-9e8f-f2ce2628e282",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model\n",
    "autoencoder = LitAutoEncoder(Encoder(), Decoder())\n",
    "optimizer = autoencoder.configure_optimizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed63ae40-cd2c-40a6-973c-b5effa9016bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xm.xla_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f8709-1594-41de-884e-c8ff6f83bfe2",
   "metadata": {},
   "source": [
    "# Training Loop Changes\n",
    "Below is the training loop. The most important steps here are:\n",
    "1. Wrapping the model with `xmp.MpModelWrapper`\n",
    "2. Initializing our XLA device with `xm.xla_device()`\n",
    "3. Sending our model to the device. \n",
    "4. Initializing parallel data loading with `MpDeviceLoader`.\n",
    "5. Replacing `optimizer.step()` lines with `xm.optimizer_step(optimizer)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "531154d3-0de1-4728-8d35-e52eead355bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    \n",
    "    autoencoder = LitAutoEncoder(Encoder(), Decoder())\n",
    "    optimizer = autoencoder.configure_optimizers()\n",
    "\n",
    "    WRAPPED_MODEL = xmp.MpModelWrapper(autoencoder)\n",
    "    \n",
    "    device = xm.xla_device()\n",
    "    model = WRAPPED_MODEL.to(device)\n",
    "    \n",
    "    para_loader = ploader.ParallelLoader(train_loader, [device])\n",
    "    para_train_loader = para_loader.per_device_loader(device)\n",
    "    \n",
    "    # para_train_loader = ploader.MpDeviceLoader(train_loader, device)\n",
    "    xm.master_print('Parallel Loader Created. Training ...')\n",
    "    \n",
    "    for batch_idx, batch in enumerate(para_train_loader):\n",
    "\n",
    "        if (batch_idx + 1) % (len(train_loader) // 10) == 0:\n",
    "            print(f'PROGRESS: Training is {((batch_idx + 1)/len(train_loader)*100):.2f}% complete...')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = autoencoder(batch, batch_idx, device)\n",
    "        loss.backward()\n",
    "        xm.optimizer_step(optimizer)\n",
    "        \n",
    "    print(\"SUCCESS: Training is 100% complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0e1bac-b7c3-4e13-bd8e-fbf179bdba05",
   "metadata": {},
   "source": [
    "# Multiprocessing Function\n",
    "Finally, we can initialize a multiprocessing function `_mp_fn` with the training loop inside, and pass the function as a callback to `xmp.spawn`. \n",
    "<br>We also need to specify the number of processes `nprocs`, which is the number of TPU cores used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95600ac4-59a7-43a9-81cf-c698e5db89fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel Loader Created. Training ...\n",
      "PROGRESS: Training is 9.81% complete...\n",
      "PROGRESS: Training is 19.62% complete...\n",
      "PROGRESS: Training is 29.42% complete...\n",
      "PROGRESS: Training is 39.23% complete...\n",
      "PROGRESS: Training is 49.04% complete...\n",
      "PROGRESS: Training is 58.85% complete...\n",
      "PROGRESS: Training is 68.66% complete...\n",
      "PROGRESS: Training is 78.46% complete...\n",
      "PROGRESS: Training is 88.27% complete...\n",
      "PROGRESS: Training is 98.08% complete...\n",
      "SUCCESS: Training is 100% complete!\n"
     ]
    }
   ],
   "source": [
    "def _mp_fn(rank, flags):\n",
    "    train_model()\n",
    "    \n",
    "FLAGS={}\n",
    "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=1, start_method='fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfffa67a-ca59-44d0-9bcf-bfe39341f7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in device=TPU:0: Cannot replicate if number of devices (1) is different from 8\n",
      "Exception in device=TPU:1: Cannot replicate if number of devices (1) is different from 8Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 330, in _mp_start_fn\n",
      "    _start_fn(index, pf_cfg, fn, args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 323, in _start_fn\n",
      "    _setup_replication()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 316, in _setup_replication\n",
      "    xm.set_replication(device, [device])\n",
      "Exception in device=TPU:2: Cannot replicate if number of devices (1) is different from 8\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/core/xla_model.py\", line 318, in set_replication\n",
      "    replication_devices = xla_replication_devices(devices)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/core/xla_model.py\", line 286, in xla_replication_devices\n",
      "    raise RuntimeError(\n",
      "Traceback (most recent call last):\n",
      "RuntimeError: Cannot replicate if number of devices (1) is different from 8\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 330, in _mp_start_fn\n",
      "    _start_fn(index, pf_cfg, fn, args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 323, in _start_fn\n",
      "    _setup_replication()\n",
      "\n",
      "Exception in device=TPU:3: Cannot replicate if number of devices (1) is different from 8  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 316, in _setup_replication\n",
      "    xm.set_replication(device, [device])\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/core/xla_model.py\", line 318, in set_replication\n",
      "    replication_devices = xla_replication_devices(devices)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/core/xla_model.py\", line 286, in xla_replication_devices\n",
      "    raise RuntimeError(\n",
      "Traceback (most recent call last):\n",
      "RuntimeError: Cannot replicate if number of devices (1) is different from 8\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 330, in _mp_start_fn\n",
      "    _start_fn(index, pf_cfg, fn, args)\n",
      "\n",
      "Exception in device=TPU:4: Cannot replicate if number of devices (1) is different from 8  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 323, in _start_fn\n",
      "    _setup_replication()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 316, in _setup_replication\n",
      "    xm.set_replication(device, [device])\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/core/xla_model.py\", line 318, in set_replication\n",
      "    replication_devices = xla_replication_devices(devices)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/core/xla_model.py\", line 286, in xla_replication_devices\n",
      "    raise RuntimeError(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 330, in _mp_start_fn\n",
      "    _start_fn(index, pf_cfg, fn, args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 323, in _start_fn\n",
      "    _setup_replication()\n",
      "RuntimeError: Cannot replicate if number of devices (1) is different from 8\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 316, in _setup_replication\n",
      "    xm.set_replication(device, [device])\n",
      "\n",
      "Exception in device=TPU:5: Cannot replicate if number of devices (1) is different from 8  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/core/xla_model.py\", line 318, in set_replication\n",
      "    replication_devices = xla_replication_devices(devices)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/core/xla_model.py\", line 286, in xla_replication_devices\n",
      "    raise RuntimeError(\n",
      "Traceback (most recent call last):\n",
      "RuntimeError: Cannot replicate if number of devices (1) is different from 8\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 330, in _mp_start_fn\n",
      "    _start_fn(index, pf_cfg, fn, args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 323, in _start_fn\n",
      "    _setup_replication()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 316, in _setup_replication\n",
      "    xm.set_replication(device, [device])\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/core/xla_model.py\", line 318, in set_replication\n",
      "    replication_devices = xla_replication_devices(devices)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/core/xla_model.py\", line 286, in xla_replication_devices\n",
      "    raise RuntimeError(\n",
      "Exception in device=TPU:6: Cannot replicate if number of devices (1) is different from 8\n",
      "RuntimeError: Cannot replicate if number of devices (1) is different from 8\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 330, in _mp_start_fn\n",
      "    _start_fn(index, pf_cfg, fn, args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 323, in _start_fn\n",
      "    _setup_replication()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 316, in _setup_replication\n",
      "    xm.set_replication(device, [device])\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/core/xla_model.py\", line 318, in set_replication\n",
      "    replication_devices = xla_replication_devices(devices)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/core/xla_model.py\", line 286, in xla_replication_devices\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot replicate if number of devices (1) is different from 8\n",
      "Exception in device=TPU:7: Cannot replicate if number of devices (1) is different from 8\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 330, in _mp_start_fn\n",
      "    _start_fn(index, pf_cfg, fn, args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 323, in _start_fn\n",
      "    _setup_replication()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 316, in _setup_replication\n",
      "    xm.set_replication(device, [device])\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/core/xla_model.py\", line 318, in set_replication\n",
      "    replication_devices = xla_replication_devices(devices)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/core/xla_model.py\", line 286, in xla_replication_devices\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot replicate if number of devices (1) is different from 8\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 330, in _mp_start_fn\n",
      "    _start_fn(index, pf_cfg, fn, args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 323, in _start_fn\n",
      "    _setup_replication()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 316, in _setup_replication\n",
      "    xm.set_replication(device, [device])\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/core/xla_model.py\", line 318, in set_replication\n",
      "    replication_devices = xla_replication_devices(devices)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/core/xla_model.py\", line 286, in xla_replication_devices\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot replicate if number of devices (1) is different from 8\n"
     ]
    },
    {
     "ename": "ProcessExitedException",
     "evalue": "process 0 terminated with exit code 17",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     train_model()\n\u001b[1;32m      4\u001b[0m FLAGS\u001b[38;5;241m=\u001b[39m{}\n\u001b[0;32m----> 5\u001b[0m \u001b[43mxmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_mp_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mFLAGS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfork\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch_xla/distributed/xla_multiprocessing.py:389\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    387\u001b[0m   _start_fn(\u001b[38;5;241m0\u001b[39m, pf_cfg, fn, args)\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 389\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_processes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_mp_start_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpf_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpf_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_devices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m      \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdaemon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdaemon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_method\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py:198\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py:149\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    141\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with signal \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    142\u001b[0m             (error_index, name),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m             signal_name\u001b[38;5;241m=\u001b[39mname\n\u001b[1;32m    147\u001b[0m         )\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    150\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with exit code \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    151\u001b[0m             (error_index, exitcode),\n\u001b[1;32m    152\u001b[0m             error_index\u001b[38;5;241m=\u001b[39merror_index,\n\u001b[1;32m    153\u001b[0m             error_pid\u001b[38;5;241m=\u001b[39mfailed_process\u001b[38;5;241m.\u001b[39mpid,\n\u001b[1;32m    154\u001b[0m             exit_code\u001b[38;5;241m=\u001b[39mexitcode\n\u001b[1;32m    155\u001b[0m         )\n\u001b[1;32m    157\u001b[0m original_trace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_queues[error_index]\u001b[38;5;241m.\u001b[39mget()\n\u001b[1;32m    158\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-- Process \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with the following error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m error_index\n",
      "\u001b[0;31mProcessExitedException\u001b[0m: process 0 terminated with exit code 17"
     ]
    }
   ],
   "source": [
    "def _mp_fn(rank, flags):\n",
    "    train_model()\n",
    "    \n",
    "FLAGS={}\n",
    "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9a3124-39da-44d7-beb5-5b9b116bd0e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
